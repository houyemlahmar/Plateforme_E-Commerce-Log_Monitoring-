# ğŸš€ DÃ©marrage du Service Ingestion (Docker)

## âœ… Architecture : Tout dans Docker

**Choix retenu** : Service d'ingestion dans Docker (meilleure pratique)
- âœ… CohÃ©rence : tous les services containerisÃ©s
- âœ… SimplicitÃ© : une seule commande `docker-compose up`
- âœ… Production-ready : mÃªme setup dev/prod
- âœ… Isolation : pas de dÃ©pendances locales

## ğŸ¯ DÃ©marrage

### Build et dÃ©marrer tous les services (incluant ingestion)
```powershell
cd C:\projet_bigdata

# Build nouveau service
docker-compose build ingestion-service

# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier
docker-compose ps
```

Vous devriez voir **8 services** actifs :
- elasticsearch
- kibana
- logstash
- mongodb
- redis
- flask-app
- celery-worker
- **ingestion-service** âœ¨ (nouveau)

## ğŸ“Š Monitoring

### Logs du service d'ingestion
```powershell
# Voir les logs en temps rÃ©el
docker-compose logs -f ingestion-service

# DerniÃ¨res 50 lignes
docker-compose logs --tail=50 ingestion-service
```

**Sortie attendue** :
```
ingestion-service | Successfully connected to Redis
ingestion-service | Successfully connected to MongoDB
ingestion-service | Ingestion Service initialized
ingestion-service | Starting ingestion service listener...
```

### VÃ©rifier la queue Redis
```powershell
docker-compose exec redis redis-cli -a changeme LLEN ingest_jobs
```

### VÃ©rifier MongoDB
```powershell
docker-compose exec mongodb mongosh -u admin -p changeme --eval "use ecommerce_logs; db.uploads.countDocuments()"
```

## ğŸ§ª Test Workflow Complet

```powershell
# 1. CrÃ©er fichier test
@"
{"timestamp":"2025-12-25T14:00:00Z","log_type":"docker_test","message":"Test from Docker"}
"@ | Out-File -FilePath uploads\docker_test.json -Encoding utf8

# 2. Upload via API
curl -X POST http://localhost:5001/api/logs/upload -F "file=@uploads\docker_test.json"

# 3. Observer le traitement (temps rÃ©el)
docker-compose logs -f ingestion-service

# Vous verrez:
# - Found 1 jobs in queue
# - Processing job abc123
# - Job completed successfully

# 4. VÃ©rifier Elasticsearch (~30s)
curl http://localhost:9200/logs-ecom-*/_search?size=5
```

## ğŸ”§ Gestion du Service

### RedÃ©marrer uniquement ingestion
```powershell
docker-compose restart ingestion-service
```

### Rebuild aprÃ¨s modification code
```powershell
docker-compose build ingestion-service
docker-compose up -d ingestion-service
```

### ArrÃªter temporairement
```powershell
docker-compose stop ingestion-service
```

### DÃ©marrer aprÃ¨s arrÃªt
```powershell
docker-compose start ingestion-service
```

## ğŸ†š Comparaison : Local vs Docker

| Aspect | Local (localhost) | Docker (redis/mongodb) |
|--------|------------------|----------------------|
| Configuration | `.env.local` nÃ©cessaire | `.env` suffit |
| DÃ©marrage | `python start_ingestion_service.py` | `docker-compose up -d` |
| Connexions | localhost:6379, localhost:27017 | redis:6379, mongodb:27017 |
| Isolation | DÃ©pendances Python locales | Container isolÃ© |
| Production | DiffÃ©rent du dev | Identique |
| Maintenance | 2 environnements Ã  gÃ©rer | 1 seul environnement |
| **Recommandation** | âŒ Dev uniquement | âœ… **Production-ready** |

## âœ¨ Avantages Solution Docker

1. **Un seul fichier** : `docker-compose.yml` contient tout
2. **Pas de configuration locale** : Fonctionne partout
3. **RedÃ©marrage auto** : `restart: unless-stopped`
4. **Logs centralisÃ©s** : `docker-compose logs`
5. **Scaling facile** : `docker-compose scale ingestion-service=3`
6. **MÃªme setup dev/prod** : Pas de surprises

## ğŸ“ Services DÃ©ployÃ©s

```
8 services actifs dans Docker :
â”œâ”€â”€ elasticsearch    (9200)
â”œâ”€â”€ kibana          (5601)
â”œâ”€â”€ logstash        (5000, 5044)
â”œâ”€â”€ mongodb         (27017)
â”œâ”€â”€ redis           (6379)
â”œâ”€â”€ flask-app       (5001)
â”œâ”€â”€ celery-worker   (4 workers)
â””â”€â”€ ingestion-service âœ¨ (traite queue Redis)
```

Tout communique via le rÃ©seau Docker `elk-network` ğŸ‰
