# ============================================
# Logstash JSON Pipeline - E-Commerce Log Processing
# File: pipeline_json.conf
# Purpose: Process JSON log files from uploads directory
# ============================================

# ============================================
# INPUT SECTION - File Watching with JSON Codec
# Monitors /data/uploads directory for new JSON files
# ============================================
input {
  # File input plugin watches for new files in specified path
  file {
    # Path to watch for JSON log files (Docker volume mount)
    path => "/data/uploads/*.json"
    
    # Start reading from beginning of file
    start_position => "beginning"
    
    # JSON codec for parsing JSON logs (one JSON object per line)
    codec => json {
      # Character set encoding
      charset => "UTF-8"
    }
    
    # Sincedb path to track file reading position
    # Prevents re-processing already read files
    sincedb_path => "/usr/share/logstash/data/sincedb_json"
    
    # File discovery mode: tail existing files and discover new ones
    mode => "tail"
    
    # Add source file metadata
    add_field => {
      "source_type" => "json_upload"
      "pipeline" => "json_processor"
    }
    
    # Tags for identification
    tags => ["json", "ecommerce", "file_input"]
  }
}

# ============================================
# FILTER SECTION - Data Processing & Transformation
# Parse dates, validate fields, enrich data
# ============================================
filter {
  # ----------------------------------------
  # Date Parsing - Handle multiple timestamp formats
  # ----------------------------------------
  if [@timestamp] {
    # If @timestamp already exists, use it
    date {
      match => ["@timestamp", "ISO8601"]
      target => "@timestamp"
      timezone => "UTC"
      tag_on_failure => ["_dateparsefailure"]
    }
  } else if [timestamp] {
    # If timestamp field exists, parse it
    date {
      match => ["timestamp",
                "ISO8601",                          # 2025-12-23T10:30:45Z
                "yyyy-MM-dd HH:mm:ss",              # 2025-12-23 10:30:45
                "yyyy-MM-dd'T'HH:mm:ss.SSSZ",       # 2025-12-23T10:30:45.123Z
                "dd/MMM/yyyy:HH:mm:ss Z",           # 23/Dec/2025:10:30:45 +0000
                "UNIX",                              # Unix timestamp
                "UNIX_MS"                            # Unix timestamp in milliseconds
      ]
      target => "@timestamp"
      timezone => "UTC"
      tag_on_failure => ["_dateparsefailure"]
    }
  } else {
    # If no timestamp field, add current time and tag
    mutate {
      add_field => { "[@timestamp]" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}" }
      add_tag => ["_no_timestamp"]
    }
  }
  
  # ----------------------------------------
  # Data Type Conversions (if fields exist)
  # ----------------------------------------
  if [amount] {
    mutate {
      convert => { "amount" => "float" }
    }
  }
  
  if [user_id] and [user_id] =~ /^USER\d+$/ {
    # Extract numeric part from USER1234 format
    mutate {
      gsub => ["user_id", "USER", ""]
    }
    mutate {
      convert => { "user_id" => "integer" }
    }
  }
  
  if [order_id] and [order_id] =~ /^[0-9]+$/ {
    mutate {
      convert => { "order_id" => "integer" }
    }
  }
  
  if [transaction_id] and [transaction_id] =~ /^TXN\d+$/ {
    # Keep as string for transaction IDs
    mutate {
      strip => ["transaction_id"]
    }
  }
  
  if [response_time] {
    mutate {
      convert => { "response_time" => "float" }
    }
  }
  
  if [fraud_score] {
    mutate {
      convert => { "fraud_score" => "integer" }
    }
  }
  
  if [status_code] {
    mutate {
      convert => { "status_code" => "integer" }
    }
  }
  
  # ----------------------------------------
  # Field Cleanup - Remove empty/null fields
  # ----------------------------------------
  mutate {
    # Remove technical metadata fields
    remove_field => [
      "host",          # Remove host metadata
      "path"           # Remove file path (sensitive information)
    ]
  }
  
  # ----------------------------------------
  # IP Address Geolocation (Optional)
  # Add geographic information based on IP
  # ----------------------------------------
  if [ip] and [ip] != "-" and [ip] != "0.0.0.0" and [ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)/ {
    geoip {
      source => "ip"
      target => "geoip"
      
      # Tag if GeoIP lookup fails
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }
  
  if [client_ip] and [client_ip] != "-" and [client_ip] != "0.0.0.0" and [client_ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)/ {
    geoip {
      source => "client_ip"
      target => "geoip"
      
      # Tag if GeoIP lookup fails
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }
  
  # ----------------------------------------
  # Data Validation - Check required fields
  # ----------------------------------------
  if ![log_type] and ![level] and ![event_type] {
    # Tag records without any log type identifier
    mutate {
      add_tag => ["_missing_log_type"]
    }
  }
  
  # ----------------------------------------
  # Enrichment - Add processing metadata
  # ----------------------------------------
  mutate {
    # Add processing timestamp
    add_field => {
      "processed_at" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
      "logstash_pipeline" => "pipeline_json"
    }
  }
}

# ============================================
# OUTPUT SECTION - Elasticsearch & Error Handling
# ============================================
output {
  # ----------------------------------------
  # SUCCESS OUTPUT - Send valid logs to Elasticsearch
  # ----------------------------------------
  if "_dateparsefailure" not in [tags] and "_jsonparsefailure" not in [tags] {
    elasticsearch {
      # Elasticsearch cluster hosts (Docker service name)
      hosts => ["elasticsearch:9200"]
      
      # Dynamic index name with date-based rotation
      # Format: logs-ecom-2025.12.23
      index => "logs-ecom-%{+YYYY.MM.dd}"
      
      # Document action type (required for Elasticsearch 8.x)
      action => "create"
      
      # Disable template management to avoid conflicts
      manage_template => false
      
      # Connection settings
      timeout => 60
    }
    
    # Console output for debugging (can be removed in production)
    stdout {
      codec => rubydebug {
        metadata => false
      }
    }
  }
  
  # ----------------------------------------
  # ERROR PIPELINE FALLBACK - Failed Records
  # Write problematic logs to Dead Letter Queue
  # ----------------------------------------
  if "_dateparsefailure" in [tags] or "_jsonparsefailure" in [tags] or "_missing_log_type" in [tags] or "_geoip_lookup_failure" in [tags] {
    # Write to dead letter queue file
    file {
      # Dead letter queue output path (mounted volume)
      path => "/data/logstash/dead_letter_queue/failed_json_logs_%{+YYYY-MM-dd}.log"
      
      # JSON format for easy reprocessing
      codec => json_lines
      
      # Flush interval (seconds)
      flush_interval => 5
      
      # File permissions
      file_mode => 0644
    }
    
    # Also send failed records to a separate Elasticsearch index for analysis
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      
      # Error index with date rotation
      index => "logs-ecom-errors-%{+YYYY.MM.dd}"
      
      # Document action type
      action => "create"
      
      # Disable template management
      manage_template => false
      
      # Connection settings
      timeout => 60
    }
    
    # Log failures to console for monitoring
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
}

# ============================================
# PIPELINE CONFIGURATION NOTES
# ============================================
# 
# Expected JSON Format (one object per line):
# {"@timestamp":"2025-12-23T10:30:45Z","log_type":"transaction","user_id":"USER1234","amount":99.99}
# 
# Supported Fields:
# - @timestamp or timestamp (required): Event timestamp
# - log_type, level, or event_type (recommended): Log category
# - user_id: User identifier (USER1234 format or integer)
# - order_id, transaction_id: Transaction identifiers
# - amount: Transaction amount (float)
# - ip, client_ip: IP address for geolocation
# - response_time, status_code: Performance metrics
# - fraud_score: Fraud detection score (0-100)
# 
# Index Patterns in Kibana:
# - logs-ecom-* (valid logs)
# - logs-ecom-errors-* (failed logs)
# 
# Dead Letter Queue Location:
# /data/logstash/dead_letter_queue/failed_json_logs_YYYY-MM-DD.log
# 
# Docker Volume Mounts Required:
# - /data/uploads (input JSON files)
# - /data/logstash/dead_letter_queue (failed records)
# 
# Error Handling:
# - Date parse failures → Tagged and sent to error index + DLQ file
# - JSON parse failures → Tagged and sent to error index + DLQ file
# - Missing log type → Tagged but still indexed to main index with warning
# - GeoIP failures → Tagged but still indexed (normal for private IPs)
# 
# ============================================
