# ============================================
# Logstash CSV Pipeline - E-Commerce Log Processing
# File: pipeline_csv.conf
# Purpose: Process CSV log files from uploads directory
# ============================================

# ============================================
# INPUT SECTION - File Watching
# Monitors /data/uploads directory for new CSV files
# ============================================
input {
  # File input plugin watches for new files in specified path
  file {
    # Path to watch for CSV log files (Docker volume mount)
    path => "/data/uploads/*.csv"
    
    # Start reading from beginning of file
    start_position => "beginning"
    
    # Sincedb path to track file reading position
    # Prevents re-processing already read files
    sincedb_path => "/usr/share/logstash/data/sincedb_csv"
    
    # File discovery mode: tail existing files and discover new ones
    mode => "tail"
    
    # Add source file metadata
    add_field => {
      "source_type" => "csv_upload"
      "pipeline" => "csv_processor"
    }
    
    # Tags for identification
    tags => ["csv", "ecommerce", "uploaded"]
  }
}

# ============================================
# FILTER SECTION - Data Processing & Transformation
# Parse CSV, convert dates, clean up fields
# ============================================
filter {
  # ----------------------------------------
  # CSV Parser - Extract fields from CSV format
  # ----------------------------------------
  csv {
    # Define CSV column names matching expected structure
    # Format: timestamp, level, service, user_id, order_id, amount, ip
    columns => [
      "timestamp",
      "level",
      "service",
      "user_id",
      "order_id",
      "amount",
      "ip"
    ]
    
    # Separator character (default: comma)
    separator => ","
    
    # Skip CSV header line if present
    skip_header => true
    
    # Remove the original message field after parsing
    remove_field => ["message"]
  }
  
  # ----------------------------------------
  # Date Conversion - Parse timestamp field
  # ----------------------------------------
  date {
    # Source field containing the timestamp
    match => ["timestamp", 
              "ISO8601",                          # 2025-12-21T10:30:45Z
              "yyyy-MM-dd HH:mm:ss",              # 2025-12-21 10:30:45
              "yyyy-MM-dd'T'HH:mm:ss.SSSZ",       # 2025-12-21T10:30:45.123Z
              "dd/MMM/yyyy:HH:mm:ss Z"            # 21/Dec/2025:10:30:45 +0000
    ]
    
    # Target field for parsed date (Elasticsearch standard)
    target => "@timestamp"
    
    # Timezone for timestamp conversion
    timezone => "UTC"
    
    # Tag if date parsing fails
    tag_on_failure => ["_dateparsefailure"]
  }
  
  # ----------------------------------------
  # Data Type Conversions
  # ----------------------------------------
  mutate {
    # Convert amount to float for numeric operations
    convert => {
      "amount" => "float"
    }
    
    # Convert order_id and user_id to integers
    convert => {
      "order_id" => "integer"
      "user_id" => "integer"
    }
    
    # Uppercase the log level (INFO, ERROR, WARN, DEBUG)
    uppercase => ["level"]
    
    # Trim whitespace from string fields
    strip => ["service", "ip"]
  }
  
  # ----------------------------------------
  # Field Cleanup - Remove empty/null fields
  # ----------------------------------------
  mutate {
    # Remove temporary fields used during processing
    remove_field => [
      "host",          # Remove host metadata (not needed in final output)
      "path"           # Remove file path (sensitive information)
    ]
  }
  
  # ----------------------------------------
  # IP Address Geolocation (Optional)
  # Add geographic information based on IP
  # ----------------------------------------
  if [ip] and [ip] != "-" and [ip] != "0.0.0.0" {
    geoip {
      source => "ip"
      target => "geoip"
      
      # Add location coordinates for Kibana maps
      add_field => {
        "[geoip][location]" => "%{[geoip][latitude]},%{[geoip][longitude]}"
      }
      
      # Tag if GeoIP lookup fails
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }
  
  # ----------------------------------------
  # Data Validation - Check required fields
  # ----------------------------------------
  if ![timestamp] or ![level] or ![service] {
    # Tag records with missing critical fields
    mutate {
      add_tag => ["_validation_failed"]
    }
  }
  
  # ----------------------------------------
  # Enrichment - Add processing metadata
  # ----------------------------------------
  mutate {
    # Add processing timestamp
    add_field => {
      "processed_at" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
      "logstash_pipeline" => "pipeline_csv"
    }
  }
}

# ============================================
# OUTPUT SECTION - Send to Elasticsearch & Dead Letter Queue
# ============================================
output {
  # ----------------------------------------
  # SUCCESS OUTPUT - Send valid logs to Elasticsearch
  # ----------------------------------------
  if "_validation_failed" not in [tags] and "_dateparsefailure" not in [tags] {
    elasticsearch {
      # Elasticsearch cluster hosts (Docker service name)
      hosts => ["elasticsearch:9200"]
      
      # Dynamic index name with date-based rotation
      # Format: logs-ecom-2025.12.21
      index => "logs-ecom-%{+YYYY.MM.dd}"
      
      # Document action type (required for Elasticsearch 8.x)
      action => "create"
      
      # Disable template management to avoid conflicts
      manage_template => false
      
      # Connection settings
      timeout => 60
    }
    
    # Console output for debugging (can be removed in production)
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
  
  # ----------------------------------------
  # DEAD LETTER QUEUE - Failed/Invalid Records
  # Write problematic logs to file for manual review
  # ----------------------------------------
  if "_validation_failed" in [tags] or "_dateparsefailure" in [tags] or "_geoip_lookup_failure" in [tags] {
    file {
      # Dead letter queue output path (mounted volume)
      path => "/data/logstash/dead_letter_queue/failed_csv_logs_%{+YYYY-MM-dd}.log"
      
      # JSON format for easy reprocessing
      codec => json_lines
      
      # Flush interval (seconds)
      flush_interval => 5
      
      # File permissions
      file_mode => 0644
    }
    
    # Log failures to console for monitoring
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
}

# ============================================
# PIPELINE CONFIGURATION NOTES
# ============================================
# 
# Expected CSV Format:
# timestamp,level,service,user_id,order_id,amount,ip
# 2025-12-21 10:30:45,INFO,payment_service,1234,5678,99.99,192.168.1.10
# 
# Index Pattern in Kibana:
# logs-ecom-*
# 
# Dead Letter Queue Location:
# /data/logstash/dead_letter_queue/failed_csv_logs_YYYY-MM-DD.log
# 
# Docker Volume Mounts Required:
# - /data/uploads (input files)
# - /data/logstash/dead_letter_queue (failed records)
# 
# ============================================
